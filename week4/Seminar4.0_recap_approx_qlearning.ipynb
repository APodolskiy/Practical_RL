{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Approximate q-learning\n",
    "\n",
    "In this notebook you will teach a lasagne neural network to do Q-learning."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Frameworks__ - we'll accept this homework in any deep learning framework. For example, it translates to TensorFlow almost line-to-line. However, we recommend you to stick to theano/lasagne unless you're certain about your skills in the framework of your choice."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "%env THEANO_FLAGS='floatX=float32'\n",
    "import os\n",
    "if type(os.environ.get(\"DISPLAY\")) is not str or len(os.environ.get(\"DISPLAY\"))==0:\n",
    "    !bash ../xvfb start\n",
    "    %env DISPLAY=:1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import gym\n",
    "import numpy as np, pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2017-09-23 18:01:18,053] Making new env: CartPole-v0\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<matplotlib.image.AxesImage at 0xcbad470>"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAXgAAAD8CAYAAAB9y7/cAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x9c4b9e8>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "env = gym.make(\"CartPole-v0\")\n",
    "env.reset()\n",
    "n_actions = env.action_space.n\n",
    "state_dim = env.observation_space.shape\n",
    "\n",
    "plt.imshow(env.render(\"rgb_array\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Approximate (deep) Q-learning: building the network\n",
    "\n",
    "In this section we will build and train naive Q-learning with theano/lasagne"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First step is initializing input variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import theano\n",
    "import theano.tensor as T\n",
    "\n",
    "#create input variables. We'll support multiple states at once\n",
    "\n",
    "\n",
    "current_states = T.matrix(\"states[batch,units]\")\n",
    "actions = T.ivector(\"action_ids[batch]\")\n",
    "rewards = T.vector(\"rewards[batch]\")\n",
    "next_states = T.matrix(\"next states[batch,units]\")\n",
    "is_end = T.ivector(\"vector[batch] where 1 means that session just ended\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import lasagne\n",
    "from lasagne.layers import *\n",
    "\n",
    "#input layer\n",
    "l_states = InputLayer((None,)+state_dim)\n",
    "\n",
    "#<Your architecture. Please start with a single-layer network>\n",
    "dense_1 = DenseLayer(l_states, num_units=50, nonlinearity=lasagne.nonlinearities.rectify, name='dense_1')\n",
    "dense_2 = DenseLayer(dense_1, num_units=60, nonlinearity=lasagne.nonlinearities.rectify, name='dense_2')\n",
    "dense_3 = DenseLayer(dense_2, num_units=35, nonlinearity=lasagne.nonlinearities.rectify, name='dense_3')\n",
    "\n",
    "#output layer\n",
    "l_qvalues = DenseLayer(dense_3, num_units=n_actions, nonlinearity=None)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Predicting Q-values for `current_states`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#get q-values for ALL actions in current_states\n",
    "predicted_qvalues = get_output(l_qvalues,{l_states:current_states})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#compiling agent's \"GetQValues\" function\n",
    "get_qvalues = theano.function(inputs=[current_states],\n",
    "                              outputs=[predicted_qvalues])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#select q-values for chosen actions\n",
    "predicted_qvalues_for_actions = predicted_qvalues[T.arange(actions.shape[0]), actions]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Loss function and `update`\n",
    "Here we write a function similar to `agent.update`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#predict q-values for next states\n",
    "predicted_next_qvalues = get_output(l_qvalues, {l_states:next_states})\n",
    "\n",
    "\n",
    "#Computing target q-values under \n",
    "gamma = 0.99\n",
    "#target_qvalues_for_actions = <target Q-values using rewards and predicted_next_qvalues>\n",
    "target_qvalues_for_actions = rewards + gamma*T.max(predicted_next_qvalues, axis=1)\n",
    "\n",
    "#zero-out q-values at the end\n",
    "target_qvalues_for_actions = (1-is_end)*target_qvalues_for_actions\n",
    "\n",
    "#don't compute gradient over target q-values (consider constant)\n",
    "target_qvalues_for_actions = theano.gradient.disconnected_grad(target_qvalues_for_actions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#mean squared error loss function\n",
    "#loss = <mean squared between target_qvalues_for_actions and predicted_qvalues_for_actions>\n",
    "loss = T.mean(lasagne.objectives.squared_error(predicted_qvalues_for_actions, target_qvalues_for_actions))\n",
    "#loss = T.mean((predicted_qvalues_for_actions - target_qvalues_for_actions)**2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#all network weights\n",
    "all_weights = get_all_params(l_qvalues,trainable=True)\n",
    "\n",
    "#network updates. Note the small learning rate (for stability)\n",
    "updates = lasagne.updates.sgd(loss,all_weights,learning_rate=1e-4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Training function that resembles agent.update(state,action,reward,next_state) \n",
    "#with 1 more argument meaning is_end\n",
    "train_step = theano.function([current_states,actions,rewards,next_states,is_end],\n",
    "                             updates=updates)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Playing the game"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "epsilon = 0.25 #initial epsilon\n",
    "\n",
    "def generate_session(t_max=1000):\n",
    "    \"\"\"play env with approximate q-learning agent and train it at the same time\"\"\"\n",
    "    \n",
    "    total_reward = 0\n",
    "    s = env.reset()\n",
    "    \n",
    "    for t in range(t_max):\n",
    "        \n",
    "        #get action q-values from the network\n",
    "        q_values = get_qvalues([s])[0] \n",
    "        \n",
    "        if np.random.random() < epsilon:\n",
    "            a = np.random.choice(n_actions)\n",
    "        else:\n",
    "            a = np.argmax(q_values)\n",
    "        \n",
    "        new_s,r,done,info = env.step(a)\n",
    "        \n",
    "        #train agent one step. Note that we use one-element arrays instead of scalars \n",
    "        #because that's what function accepts.\n",
    "        train_step([s],[a],[r],[new_s],[done])\n",
    "        \n",
    "        total_reward+=r\n",
    "        \n",
    "        s = new_s\n",
    "        if done: break\n",
    "            \n",
    "    return total_reward\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mean reward:10.870\tepsilon:0.23750\n",
      "mean reward:10.790\tepsilon:0.22562\n",
      "mean reward:10.790\tepsilon:0.21434\n",
      "mean reward:10.590\tepsilon:0.20363\n",
      "mean reward:10.370\tepsilon:0.19345\n",
      "mean reward:10.730\tepsilon:0.18377\n",
      "mean reward:10.620\tepsilon:0.17458\n",
      "mean reward:13.420\tepsilon:0.16586\n",
      "mean reward:15.400\tepsilon:0.15756\n",
      "mean reward:20.650\tepsilon:0.14968\n",
      "mean reward:37.780\tepsilon:0.14220\n",
      "mean reward:52.450\tepsilon:0.13509\n",
      "mean reward:43.110\tepsilon:0.12834\n",
      "mean reward:51.170\tepsilon:0.12192\n",
      "mean reward:95.650\tepsilon:0.11582\n",
      "mean reward:134.250\tepsilon:0.11003\n",
      "mean reward:110.060\tepsilon:0.10453\n",
      "mean reward:101.490\tepsilon:0.09930\n",
      "mean reward:26.720\tepsilon:0.09434\n",
      "mean reward:31.210\tepsilon:0.08962\n",
      "mean reward:133.670\tepsilon:0.08514\n",
      "mean reward:149.480\tepsilon:0.08088\n",
      "mean reward:137.080\tepsilon:0.07684\n",
      "mean reward:122.310\tepsilon:0.07300\n",
      "mean reward:113.740\tepsilon:0.06935\n",
      "mean reward:124.500\tepsilon:0.06588\n",
      "mean reward:163.760\tepsilon:0.06259\n",
      "mean reward:196.030\tepsilon:0.05946\n",
      "mean reward:172.060\tepsilon:0.05648\n",
      "mean reward:120.440\tepsilon:0.05366\n",
      "mean reward:113.810\tepsilon:0.05098\n",
      "mean reward:60.860\tepsilon:0.04843\n",
      "mean reward:131.170\tepsilon:0.04601\n",
      "mean reward:130.860\tepsilon:0.04371\n",
      "mean reward:131.500\tepsilon:0.04152\n",
      "mean reward:123.090\tepsilon:0.03944\n",
      "mean reward:156.580\tepsilon:0.03747\n",
      "mean reward:61.470\tepsilon:0.03560\n",
      "mean reward:184.620\tepsilon:0.03382\n",
      "mean reward:102.480\tepsilon:0.03213\n",
      "mean reward:102.950\tepsilon:0.03052\n",
      "mean reward:92.380\tepsilon:0.02900\n",
      "mean reward:166.650\tepsilon:0.02755\n",
      "mean reward:89.710\tepsilon:0.02617\n",
      "mean reward:162.880\tepsilon:0.02486\n",
      "mean reward:143.410\tepsilon:0.02362\n",
      "mean reward:129.490\tepsilon:0.02244\n",
      "mean reward:129.110\tepsilon:0.02131\n",
      "mean reward:66.790\tepsilon:0.02025\n",
      "mean reward:17.660\tepsilon:0.01924\n",
      "mean reward:19.520\tepsilon:0.01827\n",
      "mean reward:176.000\tepsilon:0.01736\n",
      "mean reward:169.300\tepsilon:0.01649\n",
      "mean reward:160.070\tepsilon:0.01567\n",
      "mean reward:152.710\tepsilon:0.01488\n",
      "mean reward:149.670\tepsilon:0.01414\n",
      "mean reward:138.970\tepsilon:0.01343\n",
      "mean reward:162.550\tepsilon:0.01276\n",
      "mean reward:180.020\tepsilon:0.01212\n",
      "mean reward:177.990\tepsilon:0.01152\n",
      "mean reward:61.160\tepsilon:0.01094\n",
      "mean reward:101.370\tepsilon:0.01039\n",
      "mean reward:146.460\tepsilon:0.00987\n",
      "mean reward:130.860\tepsilon:0.00938\n",
      "mean reward:113.800\tepsilon:0.00891\n",
      "mean reward:89.960\tepsilon:0.00847\n",
      "mean reward:134.560\tepsilon:0.00804\n",
      "mean reward:62.410\tepsilon:0.00764\n",
      "mean reward:176.590\tepsilon:0.00726\n",
      "mean reward:200.000\tepsilon:0.00690\n",
      "mean reward:155.770\tepsilon:0.00655\n",
      "mean reward:110.030\tepsilon:0.00622\n",
      "mean reward:159.350\tepsilon:0.00591\n",
      "mean reward:199.980\tepsilon:0.00562\n",
      "mean reward:186.620\tepsilon:0.00534\n",
      "mean reward:46.410\tepsilon:0.00507\n",
      "mean reward:22.330\tepsilon:0.00482\n",
      "mean reward:25.950\tepsilon:0.00457\n",
      "mean reward:82.010\tepsilon:0.00435\n",
      "mean reward:196.850\tepsilon:0.00413\n",
      "mean reward:178.480\tepsilon:0.00392\n",
      "mean reward:181.790\tepsilon:0.00373\n",
      "mean reward:191.440\tepsilon:0.00354\n",
      "mean reward:199.370\tepsilon:0.00336\n",
      "mean reward:188.800\tepsilon:0.00319\n",
      "mean reward:200.000\tepsilon:0.00304\n",
      "mean reward:200.000\tepsilon:0.00288\n",
      "mean reward:199.960\tepsilon:0.00274\n",
      "mean reward:200.000\tepsilon:0.00260\n",
      "mean reward:195.790\tepsilon:0.00247\n",
      "mean reward:53.410\tepsilon:0.00235\n",
      "mean reward:9.330\tepsilon:0.00223\n",
      "mean reward:9.300\tepsilon:0.00212\n",
      "mean reward:9.350\tepsilon:0.00201\n",
      "mean reward:63.770\tepsilon:0.00191\n",
      "mean reward:178.740\tepsilon:0.00182\n",
      "mean reward:171.070\tepsilon:0.00173\n",
      "mean reward:165.970\tepsilon:0.00164\n",
      "mean reward:159.150\tepsilon:0.00156\n",
      "mean reward:126.780\tepsilon:0.00148\n"
     ]
    }
   ],
   "source": [
    "for i in range(100):\n",
    "    \n",
    "    rewards = [generate_session() for _ in range(100)] #generate new sessions\n",
    "    \n",
    "    epsilon*=0.95\n",
    "    \n",
    "    print (\"mean reward:%.3f\\tepsilon:%.5f\"%(np.mean(rewards),epsilon))\n",
    "\n",
    "    if np.mean(rewards) > 300:\n",
    "        print (\"You Win!\")\n",
    "        break\n",
    "        \n",
    "    assert epsilon!=0, \"Please explore environment\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Video"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "epsilon=0 #Don't forget to reset epsilon back to initial value if you want to go on training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#record sessions\n",
    "import gym.wrappers\n",
    "env = gym.wrappers.Monitor(env,directory=\"videos\",force=True)\n",
    "sessions = [generate_session() for _ in range(100)]\n",
    "env.close()\n",
    "#unwrap \n",
    "env = env.env.env\n",
    "#upload to gym\n",
    "#gym.upload(\"./videos/\",api_key=\"<your_api_key>\") #you'll need me later\n",
    "\n",
    "#Warning! If you keep seeing error that reads something like\"DoubleWrapError\",\n",
    "#run env=gym.make(\"CartPole-v0\");env.reset();"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#show video\n",
    "from IPython.display import HTML\n",
    "import os\n",
    "\n",
    "video_names = list(filter(lambda s:s.endswith(\".mp4\"),os.listdir(\"./videos/\")))\n",
    "\n",
    "HTML(\"\"\"\n",
    "<video width=\"640\" height=\"480\" controls>\n",
    "  <source src=\"{}\" type=\"video/mp4\">\n",
    "</video>\n",
    "\"\"\".format(\"./videos/\"+video_names[-1])) #this may or may not be _last_ video. Try other indices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "RL_kernel",
   "language": "python",
   "name": "rl_practice"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
