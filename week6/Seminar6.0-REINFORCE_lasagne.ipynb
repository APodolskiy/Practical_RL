{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# REINFORCE in lasagne (3 pts)\n",
    "\n",
    "Just like we did before for q-learning, this time we'll design a lasagne network to learn `CartPole-v0` via policy gradient (REINFORCE).\n",
    "\n",
    "Most of the code in this notebook is taken from Seminar4.0, so you'll find it more or less familiar and even simpler."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Frameworks__ - we'll accept this homework in any deep learning framework. For example, it translates to TensorFlow almost line-to-line. However, we recommend you to stick to theano/lasagne unless you're certain about your skills in the framework of your choice."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "env: THEANO_FLAGS='floatX=float32'\n"
     ]
    }
   ],
   "source": [
    "%env THEANO_FLAGS='floatX=float32'\n",
    "import os\n",
    "if type(os.environ.get(\"DISPLAY\")) is not str or len(os.environ.get(\"DISPLAY\"))==0:\n",
    "    !bash ../xvfb start\n",
    "    %env DISPLAY=:1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2017-11-07 21:22:01,223] Making new env: CartPole-v0\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<matplotlib.image.AxesImage at 0xad36208>"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAXgAAAD8CAYAAAB9y7/cAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\ndcqAl6ROGfCS1CkDXpI69f8BAA6SuLljDfMAAAAASUVORK5CYII=\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0xa908860>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import gym\n",
    "import numpy as np, pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "env = gym.make(\"CartPole-v0\")\n",
    "env = env.env\n",
    "env.reset()\n",
    "n_actions = env.action_space.n\n",
    "state_dim = env.observation_space.shape\n",
    "\n",
    "plt.imshow(env.render(\"rgb_array\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Building the network for REINFORCE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For REINFORCE algorithm, we'll need a model that predicts action probabilities given states."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import theano\n",
    "import theano.tensor as T\n",
    "\n",
    "#create input variables. We'll support multiple states at once\n",
    "\n",
    "states = T.matrix(\"states[batch,units]\")\n",
    "actions = T.ivector(\"action_ids[batch]\")\n",
    "cumulative_rewards = T.vector(\"R[batch] = r + gamma*r' + gamma^2*r'' + ...\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import lasagne\n",
    "from lasagne.layers import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#input layer\n",
    "l_states = InputLayer((None,) + state_dim, input_var=states)\n",
    "\n",
    "\n",
    "dense_1 = DenseLayer(l_states, num_units=30, nonlinearity=lasagne.nonlinearities.rectify)\n",
    "\n",
    "dense_2 = DenseLayer(dense_1, num_units=40, nonlinearity=lasagne.nonlinearities.rectify)\n",
    "\n",
    "#output layer\n",
    "#this time we need to predict action probabilities,\n",
    "#so make sure your nonlinearity forces p>0 and sum_p = 1\n",
    "l_action_probas = DenseLayer(dense_2,\n",
    "                             num_units=n_actions,\n",
    "                             nonlinearity=lasagne.nonlinearities.softmax)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Predict function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#get probabilities of actions\n",
    "predicted_probas = get_output(l_action_probas)\n",
    "\n",
    "#predict action probability given state\n",
    "#if you use float32, set allow_input_downcast=True\n",
    "predict_proba = theano.function([states],\n",
    "                                predicted_probas)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Loss function and updates\n",
    "\n",
    "We now need to define objective and update over policy gradient.\n",
    "\n",
    "The objective function can be defined thusly:\n",
    "\n",
    "$$ J \\approx \\sum  _i log \\pi_\\theta (a_i | s_i) \\cdot R(s_i,a_i) $$\n",
    "\n",
    "When you compute gradient of that function over network weights $ \\theta $, it will become exactly the policy gradient.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#select probabilities for chosen actions, pi(a_i|s_i)\n",
    "# batch_size = actions.shape[0]\n",
    "predicted_probas_for_actions = predicted_probas[T.arange(actions.shape[0]), actions] # number of batches and selected action"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#REINFORCE objective function\n",
    "J = T.mean(T.log(predicted_probas_for_actions)*cumulative_rewards)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "neg_H = T.mean(T.sum(predicted_probas*T.log(predicted_probas), axis=-1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#all network weights\n",
    "all_weights = lasagne.layers.get_all_params(l_action_probas)\n",
    "\n",
    "#weight updates. maximize J = minimize -J\n",
    "updates = lasagne.updates.sgd(-J, all_weights, learning_rate=0.001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# with entropy\n",
    "beta = 0.01\n",
    "updates = lasagne.updates.sgd(-J + neg_H*beta, all_weights, learning_rate=0.001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "train_step = theano.function([states,actions,cumulative_rewards],\n",
    "                              updates=updates,\n",
    "                              allow_input_downcast=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Computing cumulative rewards"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_cumulative_rewards(rewards, #rewards at each step\n",
    "                           gamma = 0.99 #discount for reward\n",
    "                           ):\n",
    "    \"\"\"\n",
    "    take a list of immediate rewards r(s,a) for the whole session \n",
    "    compute cumulative rewards R(s,a) (a.k.a. G(s,a) in Sutton '16)\n",
    "    R_t = r_t + gamma*r_{t+1} + gamma^2*r_{t+2} + ...\n",
    "    \n",
    "    The simple way to compute cumulative rewards is to iterate from last to first time tick\n",
    "    and compute R_t = r_t + gamma*R_{t+1} recurrently\n",
    "    \n",
    "    You must return an array/list of cumulative rewards with as many elements as in the initial rewards.\n",
    "    \"\"\"\n",
    "    \n",
    "    cum_rewards = np.zeros(len(rewards))\n",
    "    cur_sum = 0\n",
    "    for idx, reward in enumerate(rewards[-1::-1]):\n",
    "        cur_sum = cur_sum*gamma + reward\n",
    "        cum_rewards[idx] = cur_sum\n",
    "        \n",
    "    return cum_rewards[-1::-1]\n",
    "    \n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "looks good!\n"
     ]
    }
   ],
   "source": [
    "assert len(get_cumulative_rewards(range(100))) == 100\n",
    "assert np.allclose(get_cumulative_rewards([0,0,1,0,0,1,0],gamma=0.9),[1.40049, 1.5561, 1.729, 0.81, 0.9, 1.0, 0.0])\n",
    "assert np.allclose(get_cumulative_rewards([0,0,1,-2,3,-4,0],gamma=0.5), [0.0625, 0.125, 0.25, -1.5, 1.0, -4.0, 0.0])\n",
    "assert np.allclose(get_cumulative_rewards([0,0,1,2,3,4,0],gamma=0), [0, 0, 1, 2, 3, 4, 0])\n",
    "print(\"looks good!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Playing the game"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def generate_session(t_max=1000):\n",
    "    \"\"\"play env with REINFORCE agent and train at the session end\"\"\"\n",
    "    \n",
    "    #arrays to record session\n",
    "    states,actions,rewards = [],[],[]\n",
    "    \n",
    "    s = env.reset()\n",
    "    \n",
    "    for t in range(t_max):\n",
    "        \n",
    "        #action probabilities array aka pi(a|s)\n",
    "        action_probas = predict_proba([s])[0] \n",
    "        \n",
    "        a = np.random.choice(range(n_actions), p=action_probas)\n",
    "        \n",
    "        new_s,r,done,info = env.step(a)\n",
    "        \n",
    "        #record session history to train later\n",
    "        states.append(s)\n",
    "        actions.append(a)\n",
    "        rewards.append(r)\n",
    "        \n",
    "        s = new_s\n",
    "        if done: break\n",
    "            \n",
    "    cumulative_rewards = get_cumulative_rewards(rewards)\n",
    "    train_step(states, actions, cumulative_rewards)\n",
    "            \n",
    "    return sum(rewards)\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mean reward:23.540\n",
      "mean reward:26.980\n",
      "mean reward:27.200\n",
      "mean reward:27.270\n",
      "mean reward:29.440\n",
      "mean reward:32.430\n",
      "mean reward:28.260\n",
      "mean reward:28.490\n",
      "mean reward:30.790\n",
      "mean reward:33.310\n",
      "mean reward:35.890\n",
      "mean reward:33.870\n",
      "mean reward:30.490\n",
      "mean reward:32.230\n",
      "mean reward:40.750\n",
      "mean reward:41.070\n",
      "mean reward:37.180\n",
      "mean reward:40.980\n",
      "mean reward:44.000\n",
      "mean reward:44.070\n",
      "mean reward:48.040\n",
      "mean reward:47.660\n",
      "mean reward:44.780\n",
      "mean reward:45.190\n",
      "mean reward:48.500\n",
      "mean reward:48.560\n",
      "mean reward:55.730\n",
      "mean reward:58.670\n",
      "mean reward:60.290\n",
      "mean reward:45.860\n",
      "mean reward:62.240\n",
      "mean reward:64.130\n",
      "mean reward:75.920\n",
      "mean reward:77.310\n",
      "mean reward:83.860\n",
      "mean reward:98.980\n",
      "mean reward:87.060\n",
      "mean reward:111.970\n",
      "mean reward:148.800\n",
      "mean reward:109.880\n",
      "mean reward:150.180\n",
      "mean reward:162.580\n",
      "mean reward:176.910\n",
      "mean reward:183.290\n",
      "mean reward:174.500\n",
      "mean reward:203.360\n",
      "mean reward:200.260\n",
      "mean reward:168.460\n",
      "mean reward:206.370\n",
      "mean reward:192.580\n",
      "mean reward:249.410\n",
      "mean reward:219.440\n",
      "mean reward:218.700\n",
      "mean reward:120.750\n",
      "mean reward:156.320\n",
      "mean reward:160.390\n",
      "mean reward:104.860\n",
      "mean reward:159.700\n",
      "mean reward:168.560\n",
      "mean reward:171.280\n",
      "mean reward:147.180\n",
      "mean reward:102.140\n",
      "mean reward:223.680\n",
      "mean reward:123.300\n",
      "mean reward:86.620\n",
      "mean reward:137.930\n",
      "mean reward:270.650\n",
      "mean reward:188.890\n",
      "mean reward:138.300\n",
      "mean reward:191.960\n",
      "mean reward:171.120\n",
      "mean reward:121.610\n",
      "mean reward:242.890\n",
      "mean reward:240.080\n",
      "mean reward:289.690\n",
      "mean reward:290.590\n",
      "mean reward:189.570\n",
      "mean reward:145.370\n",
      "mean reward:298.510\n",
      "mean reward:220.560\n",
      "mean reward:375.880\n",
      "You Win!\n"
     ]
    }
   ],
   "source": [
    "for i in range(100):\n",
    "    \n",
    "    rewards = [generate_session() for _ in range(100)] #generate new sessions\n",
    "    \n",
    "    print (\"mean reward:%.3f\"%(np.mean(rewards)))\n",
    "\n",
    "    if np.mean(rewards) > 300:\n",
    "        print (\"You Win!\")\n",
    "        break\n",
    "        \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Video"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2017-11-07 21:35:28,713] Making new env: CartPole-v0\n",
      "[2017-11-07 21:35:28,718] Clearing 4 monitor files from previous run (because force=True was provided)\n",
      "[2017-11-07 21:35:28,724] Starting new video recorder writing to C:\\Users\\1\\Documents\\RL_practice\\Practical_RL\\week6\\videos\\openaigym.video.0.1252.video000000.mp4\n",
      "[2017-11-07 21:35:34,177] Starting new video recorder writing to C:\\Users\\1\\Documents\\RL_practice\\Practical_RL\\week6\\videos\\openaigym.video.0.1252.video000001.mp4\n",
      "[2017-11-07 21:35:37,861] Starting new video recorder writing to C:\\Users\\1\\Documents\\RL_practice\\Practical_RL\\week6\\videos\\openaigym.video.0.1252.video000008.mp4\n",
      "[2017-11-07 21:35:41,689] Starting new video recorder writing to C:\\Users\\1\\Documents\\RL_practice\\Practical_RL\\week6\\videos\\openaigym.video.0.1252.video000027.mp4\n",
      "[2017-11-07 21:35:45,891] Starting new video recorder writing to C:\\Users\\1\\Documents\\RL_practice\\Practical_RL\\week6\\videos\\openaigym.video.0.1252.video000064.mp4\n",
      "[2017-11-07 21:35:49,573] Finished writing results. You can upload them to the scoreboard via gym.upload('C:\\\\Users\\\\1\\\\Documents\\\\RL_practice\\\\Practical_RL\\\\week6\\\\videos')\n"
     ]
    }
   ],
   "source": [
    "#record sessions\n",
    "import gym.wrappers\n",
    "env = gym.wrappers.Monitor(gym.make(\"CartPole-v0\"),directory=\"videos\",force=True)\n",
    "sessions = [generate_session() for _ in range(100)]\n",
    "env.close()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "<video width=\"640\" height=\"480\" controls>\n",
       "  <source src=\"./videos/openaigym.video.0.1252.video000064.mp4\" type=\"video/mp4\">\n",
       "</video>\n"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#show video\n",
    "from IPython.display import HTML\n",
    "import os\n",
    "\n",
    "video_names = list(filter(lambda s:s.endswith(\".mp4\"),os.listdir(\"./videos/\")))\n",
    "\n",
    "HTML(\"\"\"\n",
    "<video width=\"640\" height=\"480\" controls>\n",
    "  <source src=\"{}\" type=\"video/mp4\">\n",
    "</video>\n",
    "\"\"\".format(\"./videos/\"+video_names[-1])) #this may or may not be _last_ video. Try other indices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "RL_kernel",
   "language": "python",
   "name": "rl_practice"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
