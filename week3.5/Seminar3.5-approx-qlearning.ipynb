{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Approximate q-learning\n",
    "\n",
    "In this notebook you will teach a lasagne neural network to do Q-learning."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Frameworks__ - we'll accept this homework in any deep learning framework. For example, it translates to TensorFlow almost line-to-line. However, we recommend you to stick to theano/lasagne unless you're certain about your skills in the framework of your choice."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#XVFB will be launched if you run on a server\n",
    "import os\n",
    "if type(os.environ.get(\"DISPLAY\")) is not str or len(os.environ.get(\"DISPLAY\"))==0:\n",
    "    !bash ../xvfb start\n",
    "    %env DISPLAY=:1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import gym\n",
    "import numpy as np, pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2017-09-22 22:34:21,100] Making new env: CartPole-v0\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<matplotlib.image.AxesImage at 0xe53f780>"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAXgAAAD8CAYAAAB9y7/cAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\n6pQBL0mdMuAlqVMGvCR1yoCXpE79f3vvkrjjqz+TAAAAAElFTkSuQmCC\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0xc0e76d8>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "env = gym.make(\"CartPole-v0\")\n",
    "env.reset()\n",
    "n_actions = env.action_space.n\n",
    "state_dim = env.observation_space.shape\n",
    "\n",
    "plt.imshow(env.render(\"rgb_array\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Approximate (deep) Q-learning: building the network\n",
    "\n",
    "In this section we will build and train naive Q-learning with theano/lasagne"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First step is initializing input variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import theano\n",
    "import theano.tensor as T\n",
    "\n",
    "#create input variables. We'll support multiple states at once\n",
    "\n",
    "\n",
    "current_states = T.matrix(\"states[batch,units]\")\n",
    "actions = T.ivector(\"action_ids[batch]\")\n",
    "rewards = T.vector(\"rewards[batch]\")\n",
    "next_states = T.matrix(\"next states[batch,units]\")\n",
    "is_end = T.ivector(\"vector[batch] where 1 means that session just ended\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import lasagne\n",
    "from lasagne.layers import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#input layer\n",
    "l_states = InputLayer((None,)+state_dim)\n",
    "\n",
    "#<Your architecture. Please start with a single-layer network>\n",
    "dense_1 = DenseLayer(l_states, num_units=30, nonlinearity=lasagne.nonlinearities.rectify, name='dense_1')\n",
    "dense_2 = DenseLayer(dense_1, num_units=40, nonlinearity=lasagne.nonlinearities.rectify, name='dense_2')\n",
    "dense_3 = DenseLayer(dense_2, num_units=30, nonlinearity=lasagne.nonlinearities.rectify, name='dense_3')\n",
    "\n",
    "#output layer\n",
    "l_qvalues = DenseLayer(dense_3, num_units=n_actions, nonlinearity=None)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Predicting Q-values for `current_states`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#get q-values for ALL actions in current_states\n",
    "predicted_qvalues = get_output(l_qvalues, {l_states:current_states})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#compiling agent's \"GetQValues\" function\n",
    "#get_qvalues = <compile a function that takes current_states and returns predicted_qvalues>\n",
    "get_qvalues = theano.function(inputs=[current_states],\n",
    "                              outputs=[predicted_qvalues])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#select q-values for chosen actions\n",
    "predicted_qvalues_for_actions = predicted_qvalues[T.arange(actions.shape[0]), actions]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Loss function and `update`\n",
    "Here we write a function similar to `agent.update`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#predict q-values for next states\n",
    "predicted_next_qvalues = get_output(l_qvalues, {l_states:next_states})\n",
    "\n",
    "\n",
    "#Computing target q-values under \n",
    "gamma = 0.99\n",
    "#target_qvalues_for_actions = <target Q-values using rewards and predicted_next_qvalues>\n",
    "target_qvalues_for_actions = rewards + gamma*T.max(predicted_next_qvalues, axis=1)\n",
    "\n",
    "#zero-out q-values at the end\n",
    "target_qvalues_for_actions = (1-is_end)*target_qvalues_for_actions\n",
    "\n",
    "#don't compute gradient over target q-values (consider constant)\n",
    "target_qvalues_for_actions = theano.gradient.disconnected_grad(target_qvalues_for_actions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\n",
    "#mean squared error loss function\n",
    "#loss = <mean squared between target_qvalues_for_actions and predicted_qvalues_for_actions>\n",
    "loss = T.mean(lasagne.objectives.squared_error(predicted_qvalues_for_actions, target_qvalues_for_actions))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#all network weights\n",
    "all_weights = get_all_params(l_qvalues, trainable=True)\n",
    "\n",
    "#network updates. Note the small learning rate (for stability)\n",
    "updates = lasagne.updates.sgd(loss,all_weights,learning_rate=1e-4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Training function that resembles agent.update(state,action,reward,next_state) \n",
    "#with 1 more argument meaning is_end\n",
    "train_step = theano.function([current_states,actions,rewards,next_states,is_end],\n",
    "                             updates=updates)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Playing the game"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "epsilon = 0.25 #initial epsilon\n",
    "\n",
    "def generate_session(t_max=1000):\n",
    "    \"\"\"play env with approximate q-learning agent and train it at the same time\"\"\"\n",
    "    \n",
    "    total_reward = 0\n",
    "    s = env.reset()\n",
    "    \n",
    "    for t in range(t_max):\n",
    "        \n",
    "        #get action q-values from the network\n",
    "        q_values = get_qvalues([s])[0] \n",
    "        \n",
    "        #a = <sample action with epsilon-greedy strategy>\n",
    "        val = np.random.random()\n",
    "        if val < epsilon:\n",
    "            a = np.random.randint(n_actions)\n",
    "        else:\n",
    "            a = np.argmax(q_values)\n",
    "        \n",
    "        new_s,r,done,info = env.step(a)\n",
    "        \n",
    "        #train agent one step. Note that we use one-element arrays instead of scalars \n",
    "        #because that's what function accepts.\n",
    "        train_step([s],[a],[r],[new_s],[done])\n",
    "        \n",
    "        total_reward += r\n",
    "        \n",
    "        s = new_s\n",
    "        if done: break\n",
    "            \n",
    "    return total_reward\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mean reward:183.300\tepsilon:0.00141\n",
      "mean reward:186.860\tepsilon:0.00134\n",
      "mean reward:165.020\tepsilon:0.00127\n",
      "mean reward:200.000\tepsilon:0.00121\n",
      "mean reward:180.380\tepsilon:0.00115\n",
      "mean reward:196.330\tepsilon:0.00109\n",
      "mean reward:196.270\tepsilon:0.00103\n",
      "mean reward:184.610\tepsilon:0.00098\n",
      "mean reward:159.290\tepsilon:0.00093\n",
      "mean reward:168.370\tepsilon:0.00089\n",
      "mean reward:193.700\tepsilon:0.00084\n",
      "mean reward:95.060\tepsilon:0.00080\n",
      "mean reward:11.440\tepsilon:0.00076\n",
      "mean reward:12.070\tepsilon:0.00072\n",
      "mean reward:13.730\tepsilon:0.00069\n",
      "mean reward:12.090\tepsilon:0.00065\n",
      "mean reward:11.720\tepsilon:0.00062\n",
      "mean reward:11.440\tepsilon:0.00059\n",
      "mean reward:11.270\tepsilon:0.00056\n",
      "mean reward:11.390\tepsilon:0.00053\n",
      "mean reward:11.920\tepsilon:0.00050\n",
      "mean reward:12.540\tepsilon:0.00048\n",
      "mean reward:11.890\tepsilon:0.00045\n",
      "mean reward:15.110\tepsilon:0.00043\n",
      "mean reward:13.790\tepsilon:0.00041\n",
      "mean reward:13.680\tepsilon:0.00039\n",
      "mean reward:15.520\tepsilon:0.00037\n",
      "mean reward:20.760\tepsilon:0.00035\n",
      "mean reward:26.190\tepsilon:0.00033\n",
      "mean reward:110.860\tepsilon:0.00032\n",
      "mean reward:189.370\tepsilon:0.00030\n",
      "mean reward:171.990\tepsilon:0.00029\n",
      "mean reward:195.310\tepsilon:0.00027\n",
      "mean reward:199.890\tepsilon:0.00026\n",
      "mean reward:179.980\tepsilon:0.00025\n",
      "mean reward:200.000\tepsilon:0.00023\n",
      "mean reward:195.170\tepsilon:0.00022\n",
      "mean reward:196.510\tepsilon:0.00021\n",
      "mean reward:178.430\tepsilon:0.00020\n",
      "mean reward:194.870\tepsilon:0.00019\n",
      "mean reward:187.720\tepsilon:0.00018\n",
      "mean reward:198.140\tepsilon:0.00017\n",
      "mean reward:200.000\tepsilon:0.00016\n",
      "mean reward:171.540\tepsilon:0.00015\n",
      "mean reward:175.200\tepsilon:0.00015\n",
      "mean reward:144.450\tepsilon:0.00014\n",
      "mean reward:76.440\tepsilon:0.00013\n",
      "mean reward:198.120\tepsilon:0.00013\n",
      "mean reward:196.200\tepsilon:0.00012\n",
      "mean reward:200.000\tepsilon:0.00011\n",
      "mean reward:178.780\tepsilon:0.00011\n",
      "mean reward:187.210\tepsilon:0.00010\n",
      "mean reward:200.000\tepsilon:0.00010\n",
      "mean reward:195.620\tepsilon:0.00009\n",
      "mean reward:189.800\tepsilon:0.00009\n",
      "mean reward:143.180\tepsilon:0.00008\n",
      "mean reward:200.000\tepsilon:0.00008\n",
      "mean reward:193.090\tepsilon:0.00008\n",
      "mean reward:152.050\tepsilon:0.00007\n",
      "mean reward:150.010\tepsilon:0.00007\n",
      "mean reward:53.700\tepsilon:0.00006\n",
      "mean reward:34.890\tepsilon:0.00006\n",
      "mean reward:33.560\tepsilon:0.00006\n",
      "mean reward:49.130\tepsilon:0.00006\n",
      "mean reward:142.480\tepsilon:0.00005\n",
      "mean reward:194.090\tepsilon:0.00005\n",
      "mean reward:190.920\tepsilon:0.00005\n",
      "mean reward:200.000\tepsilon:0.00005\n",
      "mean reward:200.000\tepsilon:0.00004\n",
      "mean reward:200.000\tepsilon:0.00004\n",
      "mean reward:198.110\tepsilon:0.00004\n",
      "mean reward:200.000\tepsilon:0.00004\n",
      "mean reward:200.000\tepsilon:0.00004\n",
      "mean reward:200.000\tepsilon:0.00003\n",
      "mean reward:200.000\tepsilon:0.00003\n",
      "mean reward:200.000\tepsilon:0.00003\n",
      "mean reward:200.000\tepsilon:0.00003\n",
      "mean reward:184.830\tepsilon:0.00003\n",
      "mean reward:156.860\tepsilon:0.00003\n",
      "mean reward:139.150\tepsilon:0.00002\n",
      "mean reward:179.580\tepsilon:0.00002\n",
      "mean reward:200.000\tepsilon:0.00002\n",
      "mean reward:181.280\tepsilon:0.00002\n",
      "mean reward:196.490\tepsilon:0.00002\n",
      "mean reward:170.880\tepsilon:0.00002\n",
      "mean reward:189.190\tepsilon:0.00002\n",
      "mean reward:187.950\tepsilon:0.00002\n",
      "mean reward:185.250\tepsilon:0.00002\n",
      "mean reward:167.660\tepsilon:0.00002\n",
      "mean reward:198.840\tepsilon:0.00001\n",
      "mean reward:200.000\tepsilon:0.00001\n",
      "mean reward:178.550\tepsilon:0.00001\n",
      "mean reward:177.640\tepsilon:0.00001\n",
      "mean reward:128.800\tepsilon:0.00001\n",
      "mean reward:177.990\tepsilon:0.00001\n",
      "mean reward:188.070\tepsilon:0.00001\n",
      "mean reward:51.470\tepsilon:0.00001\n",
      "mean reward:194.110\tepsilon:0.00001\n",
      "mean reward:143.680\tepsilon:0.00001\n",
      "mean reward:200.000\tepsilon:0.00001\n"
     ]
    }
   ],
   "source": [
    "for i in range(100):\n",
    "    \n",
    "    rewards = [generate_session() for _ in range(100)] #generate new sessions\n",
    "    \n",
    "    epsilon*=0.95\n",
    "    \n",
    "    print (\"mean reward:%.3f\\tepsilon:%.5f\"%(np.mean(rewards),epsilon))\n",
    "\n",
    "    if np.mean(rewards) > 300:\n",
    "        print (\"You Win!\")\n",
    "        break\n",
    "        \n",
    "    assert epsilon != 0, \"Please explore environment\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Video"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "epsilon = 0 #Don't forget to reset epsilon back to initial value if you want to go on training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2017-09-21 22:26:29,995] Clearing 6 monitor files from previous run (because force=True was provided)\n",
      "[2017-09-21 22:26:29,999] Starting new video recorder writing to C:\\Users\\1\\Documents\\RL_practice\\Practical_RL\\week3.5\\videos\\openaigym.video.2.7964.video000000.mp4\n",
      "[2017-09-21 22:26:33,646] Starting new video recorder writing to C:\\Users\\1\\Documents\\RL_practice\\Practical_RL\\week3.5\\videos\\openaigym.video.2.7964.video000001.mp4\n",
      "[2017-09-21 22:26:37,381] Starting new video recorder writing to C:\\Users\\1\\Documents\\RL_practice\\Practical_RL\\week3.5\\videos\\openaigym.video.2.7964.video000008.mp4\n",
      "[2017-09-21 22:26:41,817] Starting new video recorder writing to C:\\Users\\1\\Documents\\RL_practice\\Practical_RL\\week3.5\\videos\\openaigym.video.2.7964.video000027.mp4\n",
      "[2017-09-21 22:26:47,063] Starting new video recorder writing to C:\\Users\\1\\Documents\\RL_practice\\Practical_RL\\week3.5\\videos\\openaigym.video.2.7964.video000064.mp4\n",
      "[2017-09-21 22:26:52,332] Finished writing results. You can upload them to the scoreboard via gym.upload('C:\\\\Users\\\\1\\\\Documents\\\\RL_practice\\\\Practical_RL\\\\week3.5\\\\videos')\n"
     ]
    }
   ],
   "source": [
    "#record sessions\n",
    "import gym.wrappers\n",
    "env = gym.wrappers.Monitor(env,directory=\"videos\",force=True)\n",
    "sessions = [generate_session() for _ in range(100)]\n",
    "env.close()\n",
    "#unwrap \n",
    "env = env.env.env\n",
    "#upload to gym\n",
    "#gym.upload(\"./videos/\",api_key=\"<your_api_key>\") #you'll need me later\n",
    "\n",
    "#Warning! If you keep seeing error that reads something like\"DoubleWrapError\",\n",
    "#run env=gym.make(\"CartPole-v0\");env.reset();"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "<video width=\"640\" height=\"480\" controls>\n",
       "  <source src=\"./videos/openaigym.video.2.7964.video000064.mp4\" type=\"video/mp4\">\n",
       "</video>\n"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "execution_count": 75,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#show video\n",
    "from IPython.display import HTML\n",
    "import os\n",
    "\n",
    "video_names = list(filter(lambda s:s.endswith(\".mp4\"),os.listdir(\"./videos/\")))\n",
    "\n",
    "HTML(\"\"\"\n",
    "<video width=\"640\" height=\"480\" controls>\n",
    "  <source src=\"{}\" type=\"video/mp4\">\n",
    "</video>\n",
    "\"\"\".format(\"./videos/\"+video_names[-1])) #this may or may not be _last_ video. Try other indices"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "### Homework\n",
    "\n",
    "Two paths lie ahead of you, and which one to take is a rightfull choice of yours.\n",
    "\n",
    "* __[recommended]__ Go deeper. Return to seminar1 and get 99% accuracy on MNIST\n",
    "* __[alternative]__ Try approximate expected-value SARSA and other algorithms and compare it with q-learning \n",
    "  * +3 points for EV-SARSA and comparison to Q-learning\n",
    "  * +2 per additional algorithm\n",
    "* __[alternative hard]__ Pick ```<your favourite env>``` and solve it, using NN.\n",
    " * LunarLander, MountainCar or Breakout (from week1 bonus)\n",
    " * LunarLander should get at least +100\n",
    " * MountainCar should get at least -200\n",
    " * You will need to somehow stabilize learning\n",
    "   \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "RL_kernel",
   "language": "python",
   "name": "rl_practice"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
